@book{neural:survival-analysis,
author = {{Miller Jr}, Rupert G},
isbn = {978-1-118-03106-3},
publisher = {John Wiley {\&} Sons},
title = {{Survival analysis}},
volume = {66},
year = {2011}
}
@article{neural:retrain,
abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
archivePrefix = {arXiv},
arxivId = {1310.1531},
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
doi = {10.1007/978-3-319-51844-2_3},
eprint = {1310.1531},
file = {:Users/jmigual/Documents/Mendeley Desktop/Donahue et al/2013 - DeCAF A Deep Convolutional Activation Feature for Generic Visual Recognition.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
title = {{DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition}},
url = {http://arxiv.org/abs/1310.1531},
year = {2013}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
doi = {10.1016/j.patrec.2014.01.008},
eprint = {1602.07261},
file = {:Users/jmigual/Documents/Mendeley Desktop/Szegedy et al/2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
pmid = {23064159},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/jmigual/Documents/Mendeley Desktop/LeCun et al/1998 - Gradient-based learning applied to document recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
year = {1998}
}
@article{Bradburn2003,
abstract = {In this series of papers, we have described a selection of statistical methods used for the initial analysis of survival time data (Clark et al, 2003), and introduced a selection of more advanced methods to deal with the situation where several factors impact on the survival process (Bradburn et al, 2003). The latter paper focused on proportional hazards (PH) and accelerated failure time (AFT) models, and we continue the series by demonstrating the application of these models in more detail. Whereas the focus of the previous paper was to outline the purpose and interpretation of statistical models for survival analysis, we concentrate here on approaches with which to undertake the actual modelling process. In other words, the aim of this paper is to promote the correct use of the models that have been suggested for the analysis of survival data. When used inappropriately, statistical models may give rise to misleading conclusions. Checking that a given model is an appropriate representation of the data is therefore an important step. Unfortunately, this is a complicated exercise, and one that has formed the subject of entire books. Here, we aim to present an overview of some of the major issues involved, and to provide general guidance when developing and applying a statistical model. We start by presenting approaches that can be used to ensure that the correct factors have been chosen. Following this, we describe some approaches that will help decide whether the statistical model adequately reflects the survivor patterns ob- served. Lastly, we describe methods to establish the validity of any assumptions the modelling process makes. We will illustrate each using the two example datasets (a lung cancer trial and an ovarian cancer dataset) that were introduced in the previous papers (Bradburn et al, 2003; Clark et al, 2003).},
author = {Bradburn, M. J. and Clark, T. G. and Love, S. B. and Altman, D. G.},
doi = {10.1038/sj.bjc.6601120},
file = {:Users/jmigual/Documents/Mendeley Desktop/Bradburn et al/2003 - Survival Analysis Part III Multivariate data analysis - Choosing a model and assessing its adequacy and fit.pdf:pdf},
isbn = {0007-0920 (Print)$\backslash$r0007-0920 (Linking)},
issn = {00070920},
journal = {British Journal of Cancer},
keywords = {AFT model,Choice of coavriates,Cox model,Goodness of fit,Model checking,Survival analysis},
number = {4},
pages = {605--611},
pmid = {12915864},
title = {{Survival Analysis Part III: Multivariate data analysis - Choosing a model and assessing its adequacy and fit}},
volume = {89},
year = {2003}
}
@article{Clark2003,
abstract = {In many cancer studies, the main outcome under assessment is the time to an event of interest. The generic name for the time is survival time, although it may be applied to the time survived from complete remission to relapse or progression as equally as to the time from diagnosis to death. If the event occurred in all individuals,manymethods of analysis would be applicable. However, it is usual that at the end of follow-up some of the individuals have not had the event of interest, and thus their true time to event is unknown. Further, survival data are rarely Normally distributed, but are skewed and comprise typically of many early events and relatively few late ones. It is these features of the data that make the special methods called survival analysis necessary. This paper is the first of a series of four articles that aim to introduce and explain the basic concepts of survival analysis. Most survival analyses in cancer journals use some or all of Kaplan Meier (KM) plots, logrank tests, and Cox (proportional hazards) regression. We will discuss the background to, and interpretation of, each of these methods but also other approaches to analysis that deserve to be used more often. In this first article, we will present the basic concepts of survival analysis, including how to produce and interpret survival curves, and how to quantify and test survival differences between two or more groups of patients. Future papers in the series cover multivariate analysis and the last paper introduces some more advanced concepts in a brief question and answer format. More detailed accounts of these methods can be found in books written specifically about survival analysis, for example, Collett (1994), Parmar and Machin (1995) and Kleinbaum (1996). In addition, individual references for the methods are presented throughout the series. Several introductory texts also describe the basis of survival analysis, for example, Altman (2003) and Piantadosi (1997).},
author = {Clark, T. G. and Bradburn, M. J. and Love, S. B. and Altman, D. G.},
doi = {10.1038/sj.bjc.6601118},
file = {:Users/jmigual/Documents/Mendeley Desktop/Clark et al/2003 - Survival Analysis Part I Basic concepts and first analyses.pdf:pdf},
isbn = {0007-0920 (Print)$\backslash$n0007-0920 (Linking)},
issn = {00070920},
journal = {British Journal of Cancer},
keywords = {Kaplan-Meier,Statistical methods,Survival analysis},
number = {2},
pages = {232--238},
pmid = {12865907},
title = {{Survival Analysis Part I: Basic concepts and first analyses}},
volume = {89},
year = {2003}
}
@article{Bradburn2003a,
abstract = {Survival analysis involves the consideration of the time between a fixed starting point (e.g. diagnosis of cancer) and a terminating event (e.g. death). The key feature that distinguishes such data from other types is that the event will not necessarily have occurred in all individuals by the time the study ends, and for these patients, their full survival times are unknown. For instance, in studies that measure the length of survival after diagnosis of cancer, it is common for a proportion of individuals to remain alive and disease-free at the end of the follow-up period, and for these patients, we know only a lower limit on their actual time to event. Thus, special methods are required for these type of data. The explanation and demonstration of some of the methods proposed to analyse such data are the basis of this series. In the first paper of this series (Clark et al, 2003), we described initial methods for analysing and summarising survival data including the definition of hazard and survival functions, and testing for a difference between two groups. We continue here by considering various statistical models and, in particular, how to estimate the effect of one or more factors that may predict survival. THE},
author = {Bradburn, M. J. and Clark, T. G. and Love, S. B. and Altman, D. G.},
doi = {10.1038/sj.bjc.6601119},
file = {:Users/jmigual/Documents/Mendeley Desktop/Bradburn et al/2003 - Survival Analysis Part II Multivariate data analysis- An introduction to concepts and methods.pdf:pdf},
isbn = {0007-0920 (Print)$\backslash$r0007-0920 (Linking)},
issn = {00070920},
journal = {British Journal of Cancer},
keywords = {AFT model,Cox model,Model selection,Survival analysis},
number = {3},
pages = {431--436},
pmid = {12888808},
title = {{Survival Analysis Part II: Multivariate data analysis- An introduction to concepts and methods}},
volume = {89},
year = {2003}
}
@article{Clark2003a,
abstract = {In the previous papers in this series (Bradburn et al, 2003a, b; Clark et al, 2003), we discussed methods for analysing survival time data, both univariate and multivariate. We have dealt with only a portion of the methods available for analysing survival time data, and in many cases, useful alternatives to (or extensions of) these methods exist. We have also left unanswered other questions regarding the design and analysis of studies that measure survival time and, in particular, dealing with situations where some standard modelling assumptions do not hold. We conclude this series by tackling these issues. These ideas are described in a question and answer format, and introductory references are provided for the reader to investigate further.},
author = {Clark, T. G. and Bradburn, M. J. and Love, S. B. and Altman, D. G.},
doi = {10.1038/sj.bjc.6601117},
file = {:Users/jmigual/Documents/Mendeley Desktop/Clark et al/2003 - Survival analysis part IV Further concepts and methods in survival analysis.pdf:pdf},
isbn = {0007-0920 (Print)$\backslash$r0007-0920 (Linking)},
issn = {00070920},
journal = {British Journal of Cancer},
keywords = {Missing data,Repeated events,Survival analysis,Validation},
number = {5},
pages = {781--786},
pmid = {12942105},
title = {{Survival analysis part IV: Further concepts and methods in survival analysis}},
volume = {89},
year = {2003}
}
@article{neural:siamese:triplet-presentation,
author = {Tech, Virginia},
file = {:Users/jmigual/Documents/Mendeley Desktop/Tech/Unknown - Siamese Triplet Networks.pdf:pdf},
title = {{Siamese / Triplet Networks}}
}
@article{neural:siamese:content-audio,
abstract = {In this paper, we focus on the problem of content-based retrieval for audio, which aims to retrieve all semantically similar audio recordings for a given audio clip query. This problem is similar to the problem of query by example of audio, which aims to retrieve media samples from a database, which are similar to the user-provided example. We propose a novel approach which encodes the audio into a vector representation using Siamese Neural Networks. The goal is to obtain an encoding similar for files belonging to the same audio class, thus allowing retrieval of semantically similar audio. Using simple similarity measures such as those based on simple euclidean distance and cosine similarity we show that these representations can be very effectively used for retrieving recordings similar in audio content.},
archivePrefix = {arXiv},
arxivId = {1710.10974},
author = {Manocha, Pranay and Badlani, Rohan and Kumar, Anurag and Shah, Ankit and Elizalde, Benjamin and Raj, Bhiksha},
eprint = {1710.10974},
file = {:Users/jmigual/Documents/Mendeley Desktop/Manocha et al/2017 - Content-based Representations of audio using Siamese neural networks.pdf:pdf},
number = {343964},
title = {{Content-based Representations of audio using Siamese neural networks}},
url = {http://arxiv.org/abs/1710.10974},
year = {2017}
}
@article{neural:siamese:pattern,
abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful represen-tations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
author = {Feragen, Aasa and Pelillo, Marcello and Loog, Marco},
doi = {10.1007/978-3-319-24261-3},
file = {:Users/jmigual/Documents/Mendeley Desktop/Feragen, Pelillo, Loog/2015 - Similarity-Based pattern recognition Third international workshop, SIMBAD 2015 copenhagen, denmark, october 12-14, 2015 proceedin.pdf:pdf},
isbn = {9783319242606},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {deep learning,metric learning,representation learning},
pages = {84--92},
title = {{Similarity-Based pattern recognition: Third international workshop, SIMBAD 2015 copenhagen, denmark, october 12-14, 2015 proceedings}},
volume = {9370},
year = {2015}
}
@article{neural:siamese:dimensionality-reduction,
abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
author = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
doi = {10.1109/CVPR.2006.100},
file = {:Users/jmigual/Documents/Mendeley Desktop/Hadsell, Chopra, LeCun/2006 - Dimensionality reduction by learning an invariant mapping.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1735--1742},
title = {{Dimensionality reduction by learning an invariant mapping}},
volume = {2},
year = {2006}
}
@misc{tool:git,
title = {{Git}},
url = {https://git-scm.com/}
}
@misc{neural:tensorflow,
author = {{Google Brain Team}},
title = {{Tensorflow}},
url = {https://www.tensorflow.org/}
}
@misc{tool:github,
title = {{GitHub}},
url = {https://github.com/}
}
@misc{neural:coursera,
title = {{Coursera}},
url = {www.coursera.org}
}
@article{medical:deep-learning-radiomics-gbm,
abstract = {Traditional radiomics models mainly rely on explicitly-designed handcrafted features from medical images. This paper aimed to investigate if deep features extracted via transfer learning can generate radiomics signatures for prediction of overall survival (OS) in patients with Glioblastoma Multiforme (GBM). This study comprised a discovery data set of 75 patients and an independent validation data set of 37 patients. A total of 1403 handcrafted features and 98304 deep features were extracted from preoperative multi-modality MR images. After feature selection, a six-deep-feature signature was constructed by using the least absolute shrinkage and selection operator (LASSO) Cox regression model. A radiomics nomogram was further presented by combining the signature and clinical risk factors such as age and Karnofsky Performance Score. Compared with traditional risk factors, the proposed signature achieved better performance for prediction of OS (C-index = 0.710, 95{\%} CI: 0.588, 0.932) and significant stratification of patients into prognostically distinct groups (P {\textless} 0.001, HR = 5.128, 95{\%} CI: 2.029, 12.960). The combined model achieved improved predictive performance (C-index = 0.739). Our study demonstrates that transfer learning-based deep features are able to generate prognostic imaging signature for OS prediction and patient stratification for GBM, indicating the potential of deep imaging feature-based biomarker in preoperative care of GBM patients.},
author = {Lao, Jiangwei and Chen, Yinsheng and Li, Zhi-Cheng Cheng and Li, Qihua and Zhang, Ji and Liu, Jing and Zhai, Guangtao},
doi = {10.1038/s41598-017-10649-8},
file = {:Users/jmigual/Documents/Mendeley Desktop/Lao et al/2017 - A Deep Learning-Based Radiomics Model for Prediction of Survival in Glioblastoma Multiforme.pdf:pdf},
isbn = {2045-2322 (Electronic)2045-2322 (Linking)},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--8},
pmid = {28871110},
publisher = {Springer US},
title = {{A Deep Learning-Based Radiomics Model for Prediction of Survival in Glioblastoma Multiforme}},
url = {http://dx.doi.org/10.1038/s41598-017-10649-8},
volume = {7},
year = {2017}
}
@article{neural:empirical-evaluation-deep-architectures,
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
address = {New York, NY, USA},
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
doi = {10.1145/1273496.1273556},
file = {:Users/jmigual/Documents/Mendeley Desktop/Larochelle et al/2007 - An empirical evaluation of deep architectures on problems with many factors of variation.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
pages = {473--480},
publisher = {ACM},
series = {ICML '07},
title = {{An empirical evaluation of deep architectures on problems with many factors of variation}},
url = {http://doi.acm.org/10.1145/1273496.1273556 http://portal.acm.org/citation.cfm?doid=1273496.1273556},
year = {2007}
}
@article{medical:radiomics-ml-classifiers,
abstract = {INTRODUCTION: "Radiomics" extracts and mines a large number of medical imaging features in a non-invasive and cost-effective way. The underlying assumption of radiomics is that these imaging features quantify phenotypic characteristics of an entire tumor. In order to enhance applicability of radiomics in clinical oncology, highly accurate and reliable machine-learning approaches are required. In this radiomic study, 13 feature selection methods and 11 machine-learning classification methods were evaluated in terms of their performance and stability for predicting overall survival in head and neck cancer patients. METHODS: Two independent head and neck cancer cohorts were investigated. Training cohort HN1 consisted of 101 head and neck cancer patients. Cohort HN2 (n = 95) was used for validation. A total of 440 radiomic features were extracted from the segmented tumor regions in CT images. Feature selection and classification methods were compared using an unbiased evaluation framework. RESULTS: We observed that the three feature selection methods minimum redundancy maximum relevance (AUC = 0.69, Stability = 0.66), mutual information feature selection (AUC = 0.66, Stability = 0.69), and conditional infomax feature extraction (AUC = 0.68, Stability = 0.7) had high prognostic performance and stability. The three classifiers BY (AUC = 0.67, RSD = 11.28), RF (AUC = 0.61, RSD = 7.36), and NN (AUC = 0.62, RSD = 10.52) also showed high prognostic performance and stability. Analysis investigating performance variability indicated that the choice of classification method is the major factor driving the performance variation (29.02{\%} of total variance). CONCLUSION: Our study identified prognostic and reliable machine-learning methods for the prediction of overall survival of head and neck cancer patients. Identification of optimal machine-learning methods for radiomics-based prognostic analyses could broaden the scope of radiomics in precision oncology and cancer care.},
author = {Parmar, Chintan and Grossmann, Patrick and Rietveld, Derek and Rietbergen, Michelle M. and Lambin, Philippe and Aerts, Hugo J. W. L.},
doi = {10.3389/fonc.2015.00272},
file = {:Users/jmigual/Documents/Mendeley Desktop/Parmar et al/2015 - Radiomic Machine-Learning Classifiers for Prognostic Biomarkers of Head and Neck Cancer.pdf:pdf;:Users/jmigual/Documents/Mendeley Desktop/Parmar et al/2015 - Radiomic Machine-Learning Classifiers for Prognostic Biomarkers of Head and Neck Cancer.pdf:pdf},
isbn = {2234-943X (Electronic) 2234-943X (Linking)},
issn = {2234-943X},
journal = {Frontiers in Oncology},
keywords = {cancer,computational science,frontiers in oncology,frontiersin,machine learning,org,quantitative imaging,radiology,radiomics,www},
number = {December},
pmid = {26697407},
publisher = {Frontiers in Oncology},
title = {{Radiomic Machine-Learning Classifiers for Prognostic Biomarkers of Head and Neck Cancer}},
url = {http://journal.frontiersin.org/Article/10.3389/fonc.2015.00272/abstract},
volume = {5},
year = {2015}
}
@article{neural:goog-le-net,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/jmigual/Documents/Mendeley Desktop/Szegedy et al/2014 - Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {arXiv:1409.4842},
pmid = {24920543},
title = {{Going Deeper with Convolutions}},
url = {https://arxiv.org/abs/1409.4842},
year = {2014}
}
@inproceedings{neural:siamese:signature,
author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S{\"{a}}ckinger, Eduard and Shah, Roopak},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jmigual/Documents/Mendeley Desktop/Bromley et al/1994 - Signature verification using a ``siamese time delay neural network.pdf:pdf},
pages = {737--744},
title = {{Signature verification using a ``siamese" time delay neural network}},
year = {1994}
}
@article{neural:siamese:similarity,
abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the},
author = {Chopra, S and Hadsell, R and Y., LeCun},
doi = {10.1109/CVPR.2005.202},
file = {:Users/jmigual/Documents/Mendeley Desktop/Chopra, Hadsell, Y/2005 - Learning a similiarty metric discriminatively, with application to face verification.pdf:pdf},
isbn = {0769523722},
issn = {10636919},
journal = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},
pages = {349--356},
title = {{Learning a similiarty metric discriminatively, with application to face verification}},
year = {2005}
}
@article{neural:hand-eye-coordination,
abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
doi = {10.1177/0278364917710318},
eprint = {1603.02199},
file = {:Users/jmigual/Documents/Mendeley Desktop/Levine et al/2017 - Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection.pdf:pdf},
isbn = {978-1-4503-3716-8},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Robotics,deep learning,neural networks},
pmid = {21156984},
title = {{Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection}},
url = {http://arxiv.org/abs/1603.02199},
year = {2017}
}
@article{medical:deep-surv,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.00931v3},
author = {Katzman, Jared L and Shaham, Uri and Cloninger, Alexander and Bates, Jonathan and Jiang, Tingting and Kluger, Yuval and Science, Computer and Steet, Prospect and Haven, New},
doi = {10.1186/s12874-018-0482-1},
eprint = {arXiv:1606.00931v3},
file = {:Users/jmigual/Documents/Mendeley Desktop/Katzman et al/2018 - DeepSurv personalized treatment recommender system using a Cox proportional hazards deep neural network.pdf:pdf},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
number = {1},
pages = {24},
title = {{DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network}},
url = {https://arxiv.org/abs/1606.00931},
volume = {18},
year = {2018}
}
@article{neural:amazon,
abstract = {Recommendation algorithms are best known for their use on e-commerce Web sites, where they use input about a customer's interests to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests, but they can also use other attributes, including items viewed, demographic data, subject interests, and favorite artists. At Amazon.com, we use recommendation algorithms to personalize the online store for each customer. The store radically changes based on customer interests, showing programming titles to a software engineer and baby toys to a new mother. There are three common approaches to solving the recommendation problem: traditional collaborative filtering, cluster models, and search-based methods. Here, we compare these methods with our algorithm, which we call item-to-item collaborative filtering. Unlike traditional collaborative filtering, our algorithm's online computation scales independently of the number of customers and number of items in the product catalog. Our algorithm produces recommendations in real-time, scales to massive data sets, and generates high quality recommendations.},
archivePrefix = {arXiv},
arxivId = {69},
author = {Linden, Greg and Smith, Brent and York, Jeremy},
doi = {10.1109/MIC.2003.1167344},
eprint = {69},
file = {:Users/jmigual/Documents/Mendeley Desktop/Linden, Smith, York/2003 - Amazon.com recommendations Item-to-item collaborative filtering.pdf:pdf},
isbn = {1089-7801},
issn = {10897801},
journal = {IEEE Internet Computing},
number = {1},
pages = {76--80},
title = {{Amazon.com recommendations: Item-to-item collaborative filtering}},
volume = {7},
year = {2003}
}
@article{Hou2017,
abstract = {In this paper, we propose an end-to-end 3D CNN for action detection and segmentation in videos. The proposed architecture is a unified deep network that is able to recognize and localize action based on 3D convolution features. A video is first divided into equal length clips and next for each clip a set of tube proposals are generated based on 3D CNN features. Finally, the tube proposals of different clips are linked together and spatio-temporal action detection is performed using these linked video proposals. This top-down action detection approach explicitly relies on a set of good tube proposals to perform well and training the bounding box regression usually requires a large number of annotated samples. To remedy this, we further extend the 3D CNN to an encoder-decoder structure and formulate the localization problem as action segmentation. The foreground regions (i.e. action regions) for each frame are segmented first then the segmented foreground maps are used to generate the bounding boxes. This bottom-up approach effectively avoids tube proposal generation by leveraging the pixel-wise annotations of segmentation. The segmentation framework also can be readily applied to a general problem of video object segmentation. Extensive experiments on several video datasets demonstrate the superior performance of our approach for action detection and video object segmentation compared to the state-of-the-arts.},
archivePrefix = {arXiv},
arxivId = {1712.01111},
author = {Hou, Rui and Chen, Chen and Shah, Mubarak},
doi = {10.1109/ICCV.2017.620},
eprint = {1712.01111},
file = {:Users/jmigual/Documents/Mendeley Desktop/Hou, Chen, Shah/2017 - An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos.pdf:pdf},
isbn = {978-1-5386-1032-9},
issn = {15505499},
number = {8},
pages = {1--15},
title = {{An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos}},
url = {http://arxiv.org/abs/1712.01111},
volume = {14},
year = {2017}
}
@article{Ranganath2016,
abstract = {The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we investigate survival analysis in the context of EHR data. We introduce deep survival analysis, a hierarchical generative approach to survival analysis. It departs from previous approaches in two primary ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it (3) scalably handles heterogeneous (continuous and discrete) data types that occur in the EHR. We validate deep survival analysis model by stratifying patients according to risk of developing coronary heart disease (CHD). Specifically, we study a dataset of 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is significantly superior in stratifying patients according to their risk.},
archivePrefix = {arXiv},
arxivId = {1608.02158},
author = {Ranganath, Rajesh and Perotte, Adler and Elhadad, No{\'{e}}mie and Blei, David},
eprint = {1608.02158},
file = {:Users/jmigual/Documents/Mendeley Desktop/Ranganath et al/2016 - Deep Survival Analysis.pdf:pdf},
month = {aug},
title = {{Deep Survival Analysis}},
url = {http://arxiv.org/abs/1608.02158},
year = {2016}
}
@article{medical:tumour-radiomics,
abstract = {Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Aerts, Hugo J.W.L. and Velazquez, Emmanuel Rios and Leijenaar, Ralph T.H. and Parmar, Chintan and Grossmann, Patrick and Cavalho, Sara and Bussink, Johan and Monshouwer, Ren{\'{e}} and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank and Rietbergen, Michelle M. and Leemans, C. Ren{\'{e}} and Dekker, Andre and Quackenbush, John and Gillies, Robert J. and Lambin, Philippe},
doi = {10.1038/ncomms5006},
eprint = {arXiv:1011.1669v3},
file = {:Users/jmigual/Documents/Mendeley Desktop/Aerts et al/2014 - Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach.pdf:pdf},
isbn = {2041-1723 (Electronic)$\backslash$r2041-1723 (Linking)},
issn = {20411723},
journal = {Nature Communications},
pmid = {24892406},
title = {{Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach}},
volume = {5},
year = {2014}
}
@article{Keshav2007,
author = {Keshav, S},
file = {:Users/jmigual/Documents/Mendeley Desktop/Keshav/2007 - How to Read a Paper.pdf:pdf},
keywords = {4,at the end of,glance over the references,hints,mentally ticking off the,ones you,paper,reading,the first pass,to answer,ve already read,you should be able},
number = {3},
pages = {83--84},
title = {{How to Read a Paper}},
volume = {37},
year = {2007}
}
@article{Esteva2017,
abstract = {Skin cancer, the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) show potential for general and highly variable tasks across many fine-grained object categories. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images-two orders of magnitude larger than previous datasets-consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
doi = {10.1038/nature21056},
file = {:Users/jmigual/Documents/Mendeley Desktop/Esteva et al/2017 - Dermatologist-level classification of skin cancer with deep neural networks.pdf:pdf},
isbn = {0028-0836},
issn = {14764687},
journal = {Nature},
number = {7639},
pages = {115--118},
pmid = {28117445},
publisher = {Nature Publishing Group},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
url = {http://dx.doi.org/10.1038/nature21056},
volume = {542},
year = {2017}
}
@article{medical:cox,
abstract = {The abalysis of censored failure times is considered . assumed on each individual are available values of on or more explanatory variables. the hazard function is taken to be function of the explanatory variables and unknown function of time. a conditional likelihood is obtained lading to ibferense about the unknown regression coffcients.},
author = {Cox, David R.},
doi = {10.1007/978-1-4612-4380-9_37},
file = {:Users/jmigual/Documents/Mendeley Desktop/Cox/1992 - Regression Models and Life-Tables.pdf:pdf},
isbn = {00359246},
issn = {00359246},
number = {2},
pages = {527--541},
pmid = {2985181},
title = {{Regression Models and Life-Tables}},
url = {http://link.springer.com/10.1007/978-1-4612-4380-9{\_}37},
volume = {34},
year = {1992}
}
@article{medical:survey-deep-learning,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.05747v2},
author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Arindra, Arnaud and Setio, Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and Laak, Jeroen A W M Van Der and Ginneken, Bram Van and Clara, I S},
eprint = {arXiv:1702.05747v2},
file = {:Users/jmigual/Documents/Mendeley Desktop/Litjens et al/2017 - A Survey on Deep Learning in Medical Image Analysis.pdf:pdf},
keywords = {convolutional neural networks,deep learning,medical imaging,survey},
title = {{A Survey on Deep Learning in Medical Image Analysis}},
year = {2017}
}
@inproceedings{VanGinneken2015,
abstract = {Convolutional neural networks (CNNs) have emerged as the most powerful technique for a range of different tasks in computer vision. Recent work suggested that CNN features are generic and can be used for classification tasks outside the exact domain for which the networks were trained. In this work we use the features from one such network, OverFeat, trained for object detection in natural images, for nodule detection in computed tomography scans. We use 865 scans from the publicly available LIDC data set, read by four thoracic radiologists. Nodule candidates are generated by a state-of-the-art nodule detection system. We extract 2D sagittal, coronal and axial patches for each nodule candidate and extract 4096 features from the penultimate layer of OverFeat and classify these with linear support vector machines. We show for various configurations that the off-the-shelf CNN features perform surprisingly well, but not as good as the dedicated detection system. When both approaches are combined, significantly better results are obtained than either approach alone. We conclude that CNN features have great potential to be used for detection tasks in volumetric medical data.},
author = {van Ginneken, Bram and Setio, Arnaud Arindra Adiyoso and Jacobs, Colin and Ciompi, Francesco},
booktitle = {2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)},
doi = {10.1109/ISBI.2015.7163869},
file = {:Users/jmigual/Documents/Mendeley Desktop/van Ginneken et al/2015 - Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans.pdf:pdf},
isbn = {978-1-4799-2374-8},
issn = {19458452},
pages = {286--289},
title = {{Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans}},
url = {http://ieeexplore.ieee.org/document/7163869/},
year = {2015}
}
@misc{neural:coursera:cnn,
author = {Deeplearning.ai},
publisher = {Coursera},
title = {{Convolutional Neural Networks}},
url = {https://www.coursera.org/learn/convolutional-neural-networks}
}
@book{neural:elements-statistical-learning,
annote = {SIGNATUR = 785.158},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
edition = {2},
isbn = {978-0-387-84858-7},
keywords = {PATTERN RECOGNITION},
publisher = {Springer},
title = {{The Elements of Statistical Learning}},
year = {2009}
}
@misc{neural:mnist,
author = {Lecun, Yann and Cortes, Corinna},
keywords = {ai,mnist,recognition},
title = {{The MNIST database of handwritten digits}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {2009}
}
@misc{neural:inception-retrain,
author = {Google},
title = {{How to Retrain Inception's Final Layer for New Categories}},
url = {https://www.tensorflow.org/tutorials/image{\_}retraining}
}
@inproceedings{neural:roc-precision-recall,
address = {New York, NY, USA},
author = {Davis, Jesse and Goadrich, Mark},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
doi = {10.1145/1143844.1143874},
file = {:Users/jmigual/Documents/Mendeley Desktop/Davis, Goadrich/2006 - The Relationship Between Precision-Recall and ROC Curves.pdf:pdf},
isbn = {1-59593-383-2},
pages = {233--240},
publisher = {ACM},
series = {ICML '06},
title = {{The Relationship Between Precision-Recall and ROC Curves}},
url = {http://doi.acm.org/10.1145/1143844.1143874},
year = {2006}
}
@misc{neural:coursera:nn-hyperparameters,
author = {Deeplearning.ai},
publisher = {Coursera},
title = {{Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization}},
url = {https://www.coursera.org/learn/deep-neural-network}
}
@article{neural:3d-cnn-crf,
author = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F J and Simpson, Joanna P and Kane, Andrew D and Menon, David K and Rueckert, Daniel and Glocker, Ben},
file = {:Users/jmigual/Documents/Mendeley Desktop/Kamnitsas et al/2017 - Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation.pdf:pdf},
journal = {Medical image analysis},
pages = {61--78},
publisher = {Elsevier},
title = {{Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation}},
volume = {36},
year = {2017}
}
@misc{medical:py-radiomics,
title = {{Radiomics feature extraction using Python}},
url = {https://github.com/Radiomics/pyradiomics}
}
@incollection{medical:ranking-ci,
author = {Steck, Harald and Krishnapuram, Balaji and Dehing-oberije, Cary and Lambin, Philippe and Raykar, Vikas C},
booktitle = {Advances in Neural Information Processing Systems 20},
editor = {Platt, J C and Koller, D and Singer, Y and Roweis, S T},
file = {:Users/jmigual/Documents/Mendeley Desktop/Steck et al/2008 - On Ranking in Survival Analysis Bounds on the Concordance Index.pdf:pdf},
pages = {1209--1216},
publisher = {Curran Associates, Inc.},
title = {{On Ranking in Survival Analysis: Bounds on the Concordance Index}},
url = {http://papers.nips.cc/paper/3375-on-ranking-in-survival-analysis-bounds-on-the-concordance-index.pdf},
year = {2008}
}
@misc{medical:deep-surv-github,
title = {{DeepSurv package}},
url = {https://github.com/jaredleekatzman/DeepSurv}
}
@misc{bhklab,
title = {{Bioinformatics and Computational Genomics Laboratory}},
url = {https://www.pmgenomics.ca/bhklab/}
}
@article{ine:salary,
author = {{Instituto Nacional de Estad{\'{i}}stica}},
title = {{Decil de salarios del empleo principial}},
url = {http://www.ine.es/prensa/epa{\_}2016{\_}d.pdf},
year = {2016}
}
@misc{neural:coursera:nn,
author = {Deeplearning.ai},
publisher = {Coursera},
title = {{Neural Networks and Deep Learning}},
url = {https://www.coursera.org/learn/neural-networks-deep-learning}
}
@misc{neural:google-assistant,
author = {Google},
title = {{Google Assistant}},
url = {https://assistant.google.com/}
}
