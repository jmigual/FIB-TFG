\section{Neural Networks}
\begin{frame}{\insertsec}
	\input{drawings/neural_network.tikz.tex}
\end{frame}

\subsection{Parameters}
\begin{frame}{\insertsubsec}
    \begin{columns}[t]
        \column{.6\textwidth}
        \begin{itemize}
            \item $l$: Current layer
            \item $w_{i, j}^{[l]}$: Weight from unit $j$ to $i$ at layer $l$
            \item $b_{i}^{[l]}$: Bias for unit $i$ at layer $l$
            \item $n^{[l]}$: Number of units in layer $l$
            \item $\bm{W}^{[l]} \in \mathbb{R}^{n^{[l]} \times n^{[l - 1]}}$: Weight matrix
            \item $\bm{b}^{[l]} \in \mathbb{R}^{n^{[l]}}$: Bias
            \item $
            \begin{aligned}[t]
            \bm{a}^{[l]} &= g(\bm{W}^{[l]}\cdot \bm{a}^{[l - 1]} + \bm{b}^{[l]}) \\
            \bm{a}^{[l]} &\in \mathbb{R}^{n^{[l]}}
            \end{aligned}
            $ Activations for layer $l$
            \item $g(x)$: Activation function
        \end{itemize}
        \column{.5\textwidth}
        \centering
        \input{drawings/neural_activations.tikz.tex}
    \end{columns}
\end{frame}
\begin{frame}
    \begin{columns}
        \column{.6\textwidth}
        \begin{block}{Cost function}
            $$
            J(\bm{W}, \bm{b}, \hat{\bm{y}}, \bm{y}) = 
            \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(\hat{\bm{y}}^{(i)}, \bm{y}^{(i)})
            $$
        \end{block}
        \begin{block}{Loss function}
            $$
            \mathcal{L}(\hat{\bm{y}}, \bm{y}) = ||\hat{\bm{y}} - \bm{y}||^2
            $$
        \end{block}
        \begin{block}{Parameters update}
            \begin{align*}
            \bm{W} &:= \bm{W} - \alpha \cdot \frac{\partial J}{\partial \bm{W}} \\
            \bm{b} &:= \bm{b} - \alpha \cdot \frac{\partial J}{\partial \bm{b}}
            \end{align*}
        \end{block}
        \column{.4\textwidth}
        To achieve better results at predicting $\hat{\bm{y}}$ 
        we have to minimize the cost function $J(\bm{W}, \bm{b}, \hat{\bm{y}}, \bm{y})$.
    \end{columns}
\end{frame}

\subsection{Regularization}
\begin{frame}{\insertsubsec}
    To prevent overfitting add the weights to the cost function so we also try to minimize
    big weights.

    \begin{align*}
        J(\bm{W}, \bm{b}, \hat{\bm{y}}, \bm{y}) &= 
        \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{\bm{y}}, \bm{y}) + 
        \frac{\lambda}{2m} \sum_{l=1}^L ||W^{[l]}||^2_F \\ 
        \underbrace{||W^{[l]}||^2_F}_{\text{Frobenius Norm}} &= \sum_i \sum_j (w_{ij}^{[l]}) \\
        \lambda &\rightarrow \text{Regularization parameter}
    \end{align*}

    It cancels some effects from some units creating a simple model.
\end{frame}
\begin{frame}[fragile]{Dropout Regularization}
    Go through each layer of the network and set some probability of elimination for each unit. 
    This is done for \textbf{each sample}.

    \begin{figure}
        \begin{verbatim}
drop = np.random.randn(*a_prev.shape) < keep_prob
a_next = np.multiply(a_prev, drop)
a_next /= keep_prob
        \end{verbatim}
        \caption{Example using \texttt{numpy}}
    \end{figure}

    \begin{alertblock}{Predictions during test time}
        No dropout is used during test time since it wouldn't make sense.
    \end{alertblock}
\end{frame}

\subsection{Training}
\begin{frame}{\insertsubsec}
    Training a NN is a minimization problem. In this case we always use gradient descent
    to find the minimum.
\end{frame}

\subsection{Hyper-parameters}
\begin{frame}{\insertsubsec}
    
\end{frame}
