\section{Neural Networks}
\begin{frame}{\insertsec}
	\input{drawings/neural_network.tikz.tex}
\end{frame}

\subsection{Parameters}
\begin{frame}{\insertsubsec}
    \begin{columns}[t]
        \column{.6\textwidth}
        \begin{itemize}
            \item $l$: Current layer
            \item $w_{i, j}^{[l]}$: Weight from unit $j$ to $i$ at layer $l$
            \item $b_{i}^{[l]}$: Bias for unit $i$ at layer $l$
            \item $n^{[l]}$: Number of units in layer $l$
            \item $W^{[l]} \in \mathbb{R}^{n^{[l]} \times n^{[l - 1]}}$: Weight matrix
            \item $\bm{b}^{[l]} \in \mathbb{R}^{n^{[l]}}$: Bias
            \item $
            \begin{aligned}[t]
                \bm{a}^{[l]} &= g(W^{[l]}\cdot \bm{a}^{[l - 1]} + \bm{b}^{[l]}) \\
                \bm{a}^{[l]} &\in \mathbb{R}^{n^{[l]}}
            \end{aligned}
            $ Activations for layer $l$
            \item $g(x)$: Activation function
        \end{itemize}
        \column{.5\textwidth}
        \centering
        \input{drawings/neural_activations.tikz.tex}
    \end{columns}
\end{frame}
\begin{frame}
    \begin{columns}
        \column{.6\textwidth}
        \begin{block}{Cost function}
            $$
            J(\bm{W}, \bm{b}, \hat{\bm{y}}, \bm{y}) = 
            \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(\hat{\bm{y}}^{(i)}, \bm{y}^{(i)})
            $$
        \end{block}
        \begin{block}{Loss function}
            $$
            \mathcal{L}(\hat{\bm{y}}, \bm{y}) = ||\hat{\bm{y}} - \bm{y}||^2
            $$
        \end{block}
        \begin{block}{Parameters update}
            \begin{align*}
            W^{[l]} &:= W^{[l]} - \alpha \cdot \frac{\partial J}{\partial W^{[l]}} \\
            \bm{b}^{[l]} &:= \bm{b}^{[l]} - \alpha \cdot \frac{\partial J}{\partial \bm{b}^{[l]}}
            \end{align*}
        \end{block}
        \column{.4\textwidth}
        To achieve better results at predicting $\hat{\bm{y}}$ 
        we have to minimize the cost function $J(\bm{W}, \bm{b}, \hat{\bm{y}}, \bm{y})$.
    \end{columns}
\end{frame}

\subsection{Regularization}
\begin{frame}{\insertsubsec}
    To prevent overfitting add the weights to the cost function so we also try to minimize
    big weights.

    \begin{align*}
        J(\bm{W}, \bm{b}, \hat{\bm{y}}, \bm{y}) &= 
        \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{\bm{y}}, \bm{y}) + 
        \frac{\lambda}{2m} \sum_{l=1}^L ||W^{[l]}||^2_F \\ 
        \underbrace{||W^{[l]}||^2_F}_{\text{Frobenius Norm}} &= \sum_i \sum_j (w_{ij}^{[l]}) \\ 
        dW^{[l]} &= \text{from backprop } + \frac{\lambda}{m} \cdot W^{[l]} \\
        \lambda &\rightarrow \text{Regularization parameter}
    \end{align*}

    It cancels some effects from some units creating a simple model.
\end{frame}
\begin{frame}[fragile]{Dropout Regularization}
    Go through each layer of the network and set some probability of elimination for each unit. 
    This is done for \textbf{each sample}.

    \begin{figure}
        \begin{verbatim}
drop = np.random.randn(*a_prev.shape) < keep_prob
a_next = np.multiply(a_prev, drop)
a_next /= keep_prob
        \end{verbatim}
        \caption{Example using \texttt{numpy}}
    \end{figure}

    \begin{alertblock}{Predictions during test time}
        No dropout is used during test time since it wouldn't make sense.
    \end{alertblock}
\end{frame}

\begin{frame}{Adam optimization algorithm}
    Training a NN is a minimization problem. In this case we always use gradient descent
    to find the minimum. Sometimes the gradient can go too fast.

    \begin{onlyenv}<1>
        \begin{align*}
            V_{dW} &= \beta_1 V_{dW} + (1 - \beta_1) \cdot dW \\
            S_{dW} &= \beta_2 S_{dW} + (1 - \beta_2) \cdot dW^2 \\
            W &:= W - \alpha \cdot \frac{V_{dW}}{\sqrt{S_{dW}} + \varepsilon}
        \end{align*}
    \end{onlyenv}
    \begin{onlyenv}<2>
        \input{drawings/momentum.tikz.tex}
    \end{onlyenv}
\end{frame}

\subsection{Hyper-parameters}
\begin{frame}{\insertsubsec}
    \begin{itemize}
        \item $m$: Mini-batch size
        \begin{itemize}
            \item $m = 1$: Stochastic gradient descent
            \item $m = $ set size: Batch gradient descent
        \end{itemize}
        \item $\alpha$: Learning rate
        \begin{itemize}
            \item Learning rate decay 
            $\alpha = \frac{1}{1 + \text{decay-rate}\times \text{epoch}} \cdot \alpha_0$
            \item Exponential decay
            $\alpha = 0.95^{\alpha_0}$

        \end{itemize}
        \item $L$: Number of layers
        \item Number of iterations
        \item Number of hidden units
    \end{itemize}
\end{frame}
